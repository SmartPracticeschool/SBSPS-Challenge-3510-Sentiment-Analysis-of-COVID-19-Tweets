{"cells": [{"metadata": {}, "cell_type": "code", "source": "!pip install sklearn\nimport nltk\nnltk.download('punkt')\n!pip install googletrans\n!pip install  GetOldTweets3\n!pip install langdetect\n!pip install nltk\n!pip install NRCLex\n!pip install Unidecode\nimport GetOldTweets3 as got\nfrom langdetect import detect\nfrom googletrans import Translator\nimport pandas as pd\nimport numpy as np\n!pip install spacy\n!pip install vaderSentiment\n\ndef get_tweets(hashtag):\n    import GetOldTweets3 as got\n    from nltk.tokenize import TweetTokenizer\n    tweetCriteria = got.manager.TweetCriteria().setQuerySearch(hashtag) \\\n        .setSince(\"2020-01-01\") \\\n        .setUntil(\"2020-05-01\") \\\n        .setMaxTweets(200)\n    # Creation of list that contains all tweets\n    tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n    # Creating list of chosen tweet data\n    text_tweets = [[tweet.text] for tweet in tweets]\n    text2 =[]\n    txt=[]\n    n=0\n    #new list to store translated tweets\n    length=len(text_tweets)\n    for i in range(length):\n    #some tweets are empty so try \n        try:\n         lang = detect(text_tweets[i][0])\n         if(lang!='en'):\n           translator = Translator()\n           translated = translator.translate(text_tweets[i][0])\n           text2.append(translated.text) \n         else:\n           text2.append(text_tweets[i][0])  \n        except:\n          n=i\n    return text2\n\nfrom nltk.tokenize import TweetTokenizer\n\ntext=[]\ntoken=[]\ntoken1=[]\ntext=get_tweets('IndiafightsCOVID19')\ntext1=get_tweets('lockdown')\nfor i in range(len(text)):\n  ttk = TweetTokenizer()\n  t=ttk.tokenize(text[i])\n  token.append(t)\nfor i in range(len(text)):\n  ttk = TweetTokenizer()\n  t=ttk.tokenize(text[i])\n  token1.append(t)\n\n\n\nimport re, string\nimport nltk\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')\nnltk.download('stopwords')\nfrom nltk.tag import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer\ndef remove_noise(tweet_tokens, stop_words = ()):\n\n    cleaned_tokens = []\n    tokens=[]\n    for token, tag in pos_tag(tweet_tokens):\n        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#\u2026]|[!*\\(\\),]'\\\n                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n\n        if tag.startswith(\"NN\"):\n            pos = 'n'\n        elif tag.startswith('VB'):\n            pos = 'v'\n        else:\n            pos = 'a'\n\n        lemmatizer = WordNetLemmatizer()\n        token = lemmatizer.lemmatize(token, pos)\n\n        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n            cleaned_tokens.append(token.lower())\n        \n    return cleaned_tokens\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\ntext=\"\"\nt=[]\nat=[]\nat1=[]\nlength=len(token)\nfor i in range(length):\n    t=(remove_noise(token[i], stop_words))\n    for i in range(len(t)): \n        at.append(t[i]) \ndf=pd.DataFrame(at,columns=['Tweets'])\nlength1=len(token1)\nfor i in range(length1):\n    t=(remove_noise(token[i], stop_words))\n    for i in range(len(t)): \n        at.append(t[i]) \ndf1=pd.DataFrame(at,columns=['Tweets'])\n\nfrom nrclex import NRCLex\nimport random\ndata= []\ni=0\nsrc=[]\nemo=[]\ndef nrc(df):\n    for i in df.index: \n             text1=df['Tweets'][i]\n             text_object = NRCLex(text1)\n             data=(text_object.top_emotions)\n             dt=random.choice(data)\n             emo.append(dt[0])\n    return emo\nret=nrc(df)\ndf2=pd.DataFrame(ret,columns=['emotions'])\ndf['emotions']=df2['emotions']\nret1=nrc(df1)\ndf2=pd.DataFrame(ret1,columns=['emotions1'])\ndf1['emotions']=df2['emotions1']\n\nfrom nltk import FreqDist\nfreq_dist_pos = FreqDist(df['emotions'])\nd=[]\nd=freq_dist_pos.most_common(10)\n#print(d)\ndf3=pd.DataFrame(d,columns=['emotion','number'])\ndf3 = df3.drop(df3[df3.emotion == 'positive'].index)\ndf3 = df3.drop(df3[df3.emotion == 'negative'].index)\n#print(df3)\nfrom project_lib import Project\nproject = Project(project_id='0f669ebe-6025-4e78-90c5-6c4f6af6a73b', project_access_token='p-2177ac7823dd38b3c5c0b5aea3410fbef3724245')\nproject.save_data(\"final_emotion_n.csv\", df3.to_csv(index=False),overwrite=True)\n\nfrom nltk import FreqDist\nfreq_dist_pos = FreqDist(df1['emotions'])\nd=[]\nd=freq_dist_pos.most_common(10)\n#print(d)\ndf4=pd.DataFrame(d,columns=['emotion','number'])\ndf4 = df4.drop(df4[df4.emotion == 'positive'].index)\ndf4 = df4.drop(df4[df4.emotion == 'negative'].index)\nfrom project_lib import Project\nproject = Project(project_id='0f669ebe-6025-4e78-90c5-6c4f6af6a73b', project_access_token='p-2177ac7823dd38b3c5c0b5aea3410fbef3724245')\nproject.save_data(\"lockdwn_emotion_n.csv\", df3.to_csv(index=False),overwrite=True)\n\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nanalyser = SentimentIntensityAnalyzer()\nlst=[]\nsent=[]\nlst1=[]\nsent1=[]\ndef sentiment_analyzer_scores(sentence):\n        score = analyser.polarity_scores(sentence)\n        \n        if score['compound'] > 0 :\n            t='positive'\n            return t\n        elif score['compound']== 0:\n            t='neutral'\n            return t\n        else:\n            t='negative'\n            return t\n\nfor i in df.index:\n    lst=sentiment_analyzer_scores(df['emotions'][i])\n    sent.append(lst)\nprint(df['emotions'])\nfrom nltk import FreqDist\nfreq_dist_pos = FreqDist(sent)\nd=[]\nd=freq_dist_pos.most_common(3)\n\ndf5=pd.DataFrame(d,columns=['sentiment','count'])\nfrom project_lib import Project\nproject = Project(project_id='0f669ebe-6025-4e78-90c5-6c4f6af6a73b', project_access_token='p-2177ac7823dd38b3c5c0b5aea3410fbef3724245')\nproject.save_data(\"final_sentiment.csv\", df5.to_csv(index=False),overwrite=True)\n\nfor i in df1.index:\n    lst1=sentiment_analyzer_scores(df1['emotions'][i])\n    sent1.append(lst1)\n#print(sent1)\nfreq_dist_pos = FreqDist(sent1)\nd1=[]\nd1=freq_dist_pos.most_common(3)\n\ndf6=pd.DataFrame(d1,columns=['sentiment','count'])\n#print(df6)\nfrom project_lib import Project\nproject = Project(project_id='0f669ebe-6025-4e78-90c5-6c4f6af6a73b', project_access_token='p-2177ac7823dd38b3c5c0b5aea3410fbef3724245')\nproject.save_data(\"lockdown_sentiment.csv\", df6.to_csv(index=False),overwrite=True)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import preprocessing\nfrom sklearn import metrics\n#creating labelEncoder\nle = preprocessing.LabelEncoder()\n# Converting string labels into numbers.\ndf['emotion_en']=le.fit_transform(df['emotions'])\n\n# Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(df['Tweets'],df['emotion_en'], test_size=0.4)\nvector = TfidfVectorizer(analyzer='word',ngram_range=(2,3), max_features=5000)\nvector.fit(df['Tweets'])\nTrain_X=vector.transform(X_train)\nTest_X=vector.transform(X_test)\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#Create KNN Classifier\nknn = KNeighborsClassifier(n_neighbors=10)\n\n#Train the model using the training sets\nknn.fit(Train_X, y_train)\n#Predict the response for test dataset\ny_pred = knn.predict(Test_X)\n\n\nyp_test = le.inverse_transform(y_pred)\nyp_train=le.inverse_transform(y_train)\nf1=pd.DataFrame(yp_train)\nf2=pd.DataFrame(yp_test)\nframes = [f1,f2]\nt1=[]\nlt=[]\nresult = pd.concat(frames)\nresult.columns=['emo']\nfor i in result.index:\n    t1=result['emo'][i]\n    lt.append(t1)\nfrom nltk import FreqDist\nfreq_dist_pos = FreqDist(result['emo'])\n#print(freq_dist_pos\nd=[]\nd=freq_dist_pos.most_common(7)", "execution_count": 130, "outputs": [{"output_type": "stream", "text": "[('negative', 1657), ('positive', 244), ('surprise', 216), ('trust', 211), ('fear', 209), ('anticip', 207), ('anger', 205)]\n", "name": "stdout"}]}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}